{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/17 17:06:26 WARN Utils: Your hostname, juan-MS-7A57 resolves to a loopback address: 127.0.1.1; using 192.168.20.22 instead (on interface enp6s0)\n",
      "23/09/17 17:06:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/17 17:06:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/17 17:06:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, 10)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream('127.0.0.1', 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ('tag', 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet = namedtuple('Tweet', fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lines.flatMap(lambda text: text.split(' '))\n",
    "    .filter(lambda word: word.lower().startswith('#'))\n",
    "    .map(lambda word: (word.lower(), 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda rec: Tweet(rec[0], rec[1]))\n",
    "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
    "    .limit(10).registerTemplate('tweets')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/17 17:06:31 ERROR JobScheduler: Error running job streaming job 1694988390000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `tweets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m      3\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     top_10_tweets \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39;49msql(\u001b[39m'\u001b[39;49m\u001b[39mSelect tag, count from tweets\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     top_10_df \u001b[39m=\u001b[39m top_10_tweets\u001b[39m.\u001b[39mtoPandas()\n\u001b[1;32m      6\u001b[0m     display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:547\u001b[0m, in \u001b[0;36mSQLContext.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sqlQuery: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    532\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m \u001b[39m    .. versionadded:: 1.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparkSession\u001b[39m.\u001b[39;49msql(sqlQuery)\n",
      "File \u001b[0;32m~/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[39m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `tweets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/17 17:06:40 ERROR JobScheduler: Error running job streaming job 1694988400000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:06:50 ERROR JobScheduler: Error running job streaming job 1694988410000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:06:58 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:06:58 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:06:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:06:59 WARN BlockManager: Block input-0-1694988418800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:07:00 ERROR JobScheduler: Error running job streaming job 1694988420000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:07:10 ERROR JobScheduler: Error running job streaming job 1694988430000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:07:20 ERROR JobScheduler: Error running job streaming job 1694988440000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:07:30 ERROR JobScheduler: Error running job streaming job 1694988450000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:07:30 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:07:30 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:07:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:07:31 WARN BlockManager: Block input-0-1694988450800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:07:40 ERROR JobScheduler: Error running job streaming job 1694988460000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:07:50 ERROR JobScheduler: Error running job streaming job 1694988470000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:08:00 ERROR JobScheduler: Error running job streaming job 1694988480000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:08:02 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:08:02 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:08:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:08:03 WARN BlockManager: Block input-0-1694988482800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:08:10 ERROR JobScheduler: Error running job streaming job 1694988490000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:08:20 ERROR JobScheduler: Error running job streaming job 1694988500000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:08:30 ERROR JobScheduler: Error running job streaming job 1694988510000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:08:34 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:08:34 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:08:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:08:35 WARN BlockManager: Block input-0-1694988514800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:08:40 ERROR JobScheduler: Error running job streaming job 1694988520000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:08:50 ERROR JobScheduler: Error running job streaming job 1694988530000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:09:00 ERROR JobScheduler: Error running job streaming job 1694988540000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:09:06 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:09:06 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:09:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:09:07 WARN BlockManager: Block input-0-1694988546800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:09:10 ERROR JobScheduler: Error running job streaming job 1694988550000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:09:20 ERROR JobScheduler: Error running job streaming job 1694988560000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:09:30 ERROR JobScheduler: Error running job streaming job 1694988570000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:09:38 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:09:38 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:09:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:09:39 WARN BlockManager: Block input-0-1694988578800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:09:40 ERROR JobScheduler: Error running job streaming job 1694988580000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:09:50 ERROR JobScheduler: Error running job streaming job 1694988590000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:10:00 ERROR JobScheduler: Error running job streaming job 1694988600000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:10:10 ERROR JobScheduler: Error running job streaming job 1694988610000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:10:11 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:10:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:10:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:10:11 WARN BlockManager: Block input-0-1694988610800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:10:20 ERROR JobScheduler: Error running job streaming job 1694988620000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:10:30 ERROR JobScheduler: Error running job streaming job 1694988630000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:10:40 ERROR JobScheduler: Error running job streaming job 1694988640000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:10:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:10:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:10:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:10:43 WARN BlockManager: Block input-0-1694988643000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:10:50 ERROR JobScheduler: Error running job streaming job 1694988650000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:11:00 ERROR JobScheduler: Error running job streaming job 1694988660000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:11:10 ERROR JobScheduler: Error running job streaming job 1694988670000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:11:15 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:11:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:11:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:11:15 WARN BlockManager: Block input-0-1694988675000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:11:20 ERROR JobScheduler: Error running job streaming job 1694988680000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:11:30 ERROR JobScheduler: Error running job streaming job 1694988690000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:11:40 ERROR JobScheduler: Error running job streaming job 1694988700000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:11:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:11:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:11:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:11:47 WARN BlockManager: Block input-0-1694988707000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:11:50 ERROR JobScheduler: Error running job streaming job 1694988710000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:00 ERROR JobScheduler: Error running job streaming job 1694988720000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:10 ERROR JobScheduler: Error running job streaming job 1694988730000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:19 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:12:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:12:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:12:19 WARN BlockManager: Block input-0-1694988739000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:12:20 ERROR JobScheduler: Error running job streaming job 1694988740000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:30 ERROR JobScheduler: Error running job streaming job 1694988750000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:40 ERROR JobScheduler: Error running job streaming job 1694988760000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:50 ERROR JobScheduler: Error running job streaming job 1694988770000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:12:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:12:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:12:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:12:51 WARN BlockManager: Block input-0-1694988771000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:13:00 ERROR JobScheduler: Error running job streaming job 1694988780000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:13:10 ERROR JobScheduler: Error running job streaming job 1694988790000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:13:20 ERROR JobScheduler: Error running job streaming job 1694988800000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:13:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:13:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:13:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:13:23 WARN BlockManager: Block input-0-1694988803000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:13:30 ERROR JobScheduler: Error running job streaming job 1694988810000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:13:40 ERROR JobScheduler: Error running job streaming job 1694988820000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:13:50 ERROR JobScheduler: Error running job streaming job 1694988830000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:13:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:13:55 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:13:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:13:55 WARN BlockManager: Block input-0-1694988835000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:14:00 ERROR JobScheduler: Error running job streaming job 1694988840000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:14:10 ERROR JobScheduler: Error running job streaming job 1694988850000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:14:20 ERROR JobScheduler: Error running job streaming job 1694988860000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:14:27 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:14:27 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:14:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:14:27 WARN BlockManager: Block input-0-1694988867200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:14:30 ERROR JobScheduler: Error running job streaming job 1694988870000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:14:40 ERROR JobScheduler: Error running job streaming job 1694988880000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:14:50 ERROR JobScheduler: Error running job streaming job 1694988890000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:14:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:14:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:14:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:14:59 WARN BlockManager: Block input-0-1694988899200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:15:00 ERROR JobScheduler: Error running job streaming job 1694988900000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:15:10 ERROR JobScheduler: Error running job streaming job 1694988910000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:15:20 ERROR JobScheduler: Error running job streaming job 1694988920000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:15:30 ERROR JobScheduler: Error running job streaming job 1694988930000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:15:31 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:15:31 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:15:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:15:31 WARN BlockManager: Block input-0-1694988931200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:15:40 ERROR JobScheduler: Error running job streaming job 1694988940000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:15:50 ERROR JobScheduler: Error running job streaming job 1694988950000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:16:00 ERROR JobScheduler: Error running job streaming job 1694988960000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:16:03 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:16:03 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:16:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:16:03 WARN BlockManager: Block input-0-1694988963200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:16:10 ERROR JobScheduler: Error running job streaming job 1694988970000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:16:20 ERROR JobScheduler: Error running job streaming job 1694988980000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:16:30 ERROR JobScheduler: Error running job streaming job 1694988990000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:16:35 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:16:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:16:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:16:35 WARN BlockManager: Block input-0-1694988995200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:16:40 ERROR JobScheduler: Error running job streaming job 1694989000000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:16:50 ERROR JobScheduler: Error running job streaming job 1694989010000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:17:00 ERROR JobScheduler: Error running job streaming job 1694989020000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:17:07 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:17:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:17:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:17:07 WARN BlockManager: Block input-0-1694989027200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:17:10 ERROR JobScheduler: Error running job streaming job 1694989030000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:17:20 ERROR JobScheduler: Error running job streaming job 1694989040000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:17:30 ERROR JobScheduler: Error running job streaming job 1694989050000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:17:39 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:17:39 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:17:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:17:39 WARN BlockManager: Block input-0-1694989059200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:17:40 ERROR JobScheduler: Error running job streaming job 1694989060000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:17:50 ERROR JobScheduler: Error running job streaming job 1694989070000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:18:00 ERROR JobScheduler: Error running job streaming job 1694989080000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:18:10 ERROR JobScheduler: Error running job streaming job 1694989090000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:18:11 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:18:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:18:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:18:11 WARN BlockManager: Block input-0-1694989091400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:18:20 ERROR JobScheduler: Error running job streaming job 1694989100000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:18:30 ERROR JobScheduler: Error running job streaming job 1694989110000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:18:40 ERROR JobScheduler: Error running job streaming job 1694989120000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:18:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:18:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:18:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:18:43 WARN BlockManager: Block input-0-1694989123400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:18:50 ERROR JobScheduler: Error running job streaming job 1694989130000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:19:00 ERROR JobScheduler: Error running job streaming job 1694989140000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:19:10 ERROR JobScheduler: Error running job streaming job 1694989150000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:19:15 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:19:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:19:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:19:15 WARN BlockManager: Block input-0-1694989155400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:19:20 ERROR JobScheduler: Error running job streaming job 1694989160000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:19:30 ERROR JobScheduler: Error running job streaming job 1694989170000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:19:40 ERROR JobScheduler: Error running job streaming job 1694989180000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:19:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:19:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:19:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:19:47 WARN BlockManager: Block input-0-1694989187400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:19:50 ERROR JobScheduler: Error running job streaming job 1694989190000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:00 ERROR JobScheduler: Error running job streaming job 1694989200000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:10 ERROR JobScheduler: Error running job streaming job 1694989210000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:19 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:20:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:20:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:20:19 WARN BlockManager: Block input-0-1694989219400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:20:20 ERROR JobScheduler: Error running job streaming job 1694989220000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:30 ERROR JobScheduler: Error running job streaming job 1694989230000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:40 ERROR JobScheduler: Error running job streaming job 1694989240000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:50 ERROR JobScheduler: Error running job streaming job 1694989250000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:20:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:20:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:20:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:20:51 WARN BlockManager: Block input-0-1694989251400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:21:00 ERROR JobScheduler: Error running job streaming job 1694989260000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:21:10 ERROR JobScheduler: Error running job streaming job 1694989270000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:21:20 ERROR JobScheduler: Error running job streaming job 1694989280000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:21:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:21:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:21:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:21:23 WARN BlockManager: Block input-0-1694989283400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:21:30 ERROR JobScheduler: Error running job streaming job 1694989290000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:21:40 ERROR JobScheduler: Error running job streaming job 1694989300000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:21:50 ERROR JobScheduler: Error running job streaming job 1694989310000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:21:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:21:55 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:21:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:21:55 WARN BlockManager: Block input-0-1694989315600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:22:00 ERROR JobScheduler: Error running job streaming job 1694989320000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:22:10 ERROR JobScheduler: Error running job streaming job 1694989330000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:22:20 ERROR JobScheduler: Error running job streaming job 1694989340000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:22:27 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:22:27 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:22:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:22:27 WARN BlockManager: Block input-0-1694989347600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:22:30 ERROR JobScheduler: Error running job streaming job 1694989350000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:22:40 ERROR JobScheduler: Error running job streaming job 1694989360000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:22:50 ERROR JobScheduler: Error running job streaming job 1694989370000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:22:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:22:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:22:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:22:59 WARN BlockManager: Block input-0-1694989379600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:23:00 ERROR JobScheduler: Error running job streaming job 1694989380000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:23:10 ERROR JobScheduler: Error running job streaming job 1694989390000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:23:20 ERROR JobScheduler: Error running job streaming job 1694989400000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:23:30 ERROR JobScheduler: Error running job streaming job 1694989410000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:23:31 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:23:31 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:23:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:23:31 WARN BlockManager: Block input-0-1694989411600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:23:40 ERROR JobScheduler: Error running job streaming job 1694989420000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:23:50 ERROR JobScheduler: Error running job streaming job 1694989430000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:24:00 ERROR JobScheduler: Error running job streaming job 1694989440000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:24:03 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:24:03 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:24:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:24:03 WARN BlockManager: Block input-0-1694989443600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:24:10 ERROR JobScheduler: Error running job streaming job 1694989450000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:24:20 ERROR JobScheduler: Error running job streaming job 1694989460000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:24:30 ERROR JobScheduler: Error running job streaming job 1694989470000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:24:35 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:24:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:24:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:24:35 WARN BlockManager: Block input-0-1694989475600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:24:40 ERROR JobScheduler: Error running job streaming job 1694989480000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:24:50 ERROR JobScheduler: Error running job streaming job 1694989490000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:25:00 ERROR JobScheduler: Error running job streaming job 1694989500000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:25:07 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:25:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:25:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:25:07 WARN BlockManager: Block input-0-1694989507600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:25:10 ERROR JobScheduler: Error running job streaming job 1694989510000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:25:20 ERROR JobScheduler: Error running job streaming job 1694989520000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:25:30 ERROR JobScheduler: Error running job streaming job 1694989530000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:25:39 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:25:39 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:25:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:25:39 WARN BlockManager: Block input-0-1694989539600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:25:40 ERROR JobScheduler: Error running job streaming job 1694989540000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:25:50 ERROR JobScheduler: Error running job streaming job 1694989550000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:26:00 ERROR JobScheduler: Error running job streaming job 1694989560000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:26:10 ERROR JobScheduler: Error running job streaming job 1694989570000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:26:11 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:26:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:26:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:26:12 WARN BlockManager: Block input-0-1694989571800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:26:20 ERROR JobScheduler: Error running job streaming job 1694989580000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:26:30 ERROR JobScheduler: Error running job streaming job 1694989590000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:26:40 ERROR JobScheduler: Error running job streaming job 1694989600000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:26:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:26:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:26:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:26:44 WARN BlockManager: Block input-0-1694989603800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:26:50 ERROR JobScheduler: Error running job streaming job 1694989610000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:27:00 ERROR JobScheduler: Error running job streaming job 1694989620000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:27:10 ERROR JobScheduler: Error running job streaming job 1694989630000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:27:15 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:27:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:27:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:27:16 WARN BlockManager: Block input-0-1694989635800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:27:20 ERROR JobScheduler: Error running job streaming job 1694989640000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:27:30 ERROR JobScheduler: Error running job streaming job 1694989650000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:27:40 ERROR JobScheduler: Error running job streaming job 1694989660000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:27:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:27:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:27:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:27:48 WARN BlockManager: Block input-0-1694989667800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:27:50 ERROR JobScheduler: Error running job streaming job 1694989670000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:00 ERROR JobScheduler: Error running job streaming job 1694989680000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:10 ERROR JobScheduler: Error running job streaming job 1694989690000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:19 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:28:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:28:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:28:20 WARN BlockManager: Block input-0-1694989699800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:28:20 ERROR JobScheduler: Error running job streaming job 1694989700000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:30 ERROR JobScheduler: Error running job streaming job 1694989710000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:40 ERROR JobScheduler: Error running job streaming job 1694989720000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:50 ERROR JobScheduler: Error running job streaming job 1694989730000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:28:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:28:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:28:52 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:28:52 WARN BlockManager: Block input-0-1694989731800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:29:00 ERROR JobScheduler: Error running job streaming job 1694989740000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:29:10 ERROR JobScheduler: Error running job streaming job 1694989750000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:29:20 ERROR JobScheduler: Error running job streaming job 1694989760000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:29:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:29:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:29:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:29:24 WARN BlockManager: Block input-0-1694989763800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:29:30 ERROR JobScheduler: Error running job streaming job 1694989770000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:29:40 ERROR JobScheduler: Error running job streaming job 1694989780000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:29:50 ERROR JobScheduler: Error running job streaming job 1694989790000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:29:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:29:56 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:29:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:29:56 WARN BlockManager: Block input-0-1694989795800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:30:00 ERROR JobScheduler: Error running job streaming job 1694989800000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:30:10 ERROR JobScheduler: Error running job streaming job 1694989810000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:30:20 ERROR JobScheduler: Error running job streaming job 1694989820000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:30:28 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:30:28 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:30:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:30:28 WARN BlockManager: Block input-0-1694989828000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:30:30 ERROR JobScheduler: Error running job streaming job 1694989830000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:30:40 ERROR JobScheduler: Error running job streaming job 1694989840000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:30:50 ERROR JobScheduler: Error running job streaming job 1694989850000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:31:00 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:31:00 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:31:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:31:00 WARN BlockManager: Block input-0-1694989860000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:31:00 ERROR JobScheduler: Error running job streaming job 1694989860000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:31:10 ERROR JobScheduler: Error running job streaming job 1694989870000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:31:20 ERROR JobScheduler: Error running job streaming job 1694989880000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:31:30 ERROR JobScheduler: Error running job streaming job 1694989890000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:31:32 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:31:32 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:31:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:31:32 WARN BlockManager: Block input-0-1694989892000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:31:40 ERROR JobScheduler: Error running job streaming job 1694989900000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:31:50 ERROR JobScheduler: Error running job streaming job 1694989910000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:32:00 ERROR JobScheduler: Error running job streaming job 1694989920000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:32:04 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:32:04 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:32:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:32:04 WARN BlockManager: Block input-0-1694989924000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:32:10 ERROR JobScheduler: Error running job streaming job 1694989930000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:32:20 ERROR JobScheduler: Error running job streaming job 1694989940000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:32:30 ERROR JobScheduler: Error running job streaming job 1694989950000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:32:36 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:32:36 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:32:36 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:32:36 WARN BlockManager: Block input-0-1694989956000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:32:40 ERROR JobScheduler: Error running job streaming job 1694989960000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:32:50 ERROR JobScheduler: Error running job streaming job 1694989970000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:33:00 ERROR JobScheduler: Error running job streaming job 1694989980000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:33:08 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:33:08 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:33:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:33:08 WARN BlockManager: Block input-0-1694989988000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:33:10 ERROR JobScheduler: Error running job streaming job 1694989990000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:33:20 ERROR JobScheduler: Error running job streaming job 1694990000000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:33:30 ERROR JobScheduler: Error running job streaming job 1694990010000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:33:40 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:33:40 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:33:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:33:40 WARN BlockManager: Block input-0-1694990020000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:33:40 ERROR JobScheduler: Error running job streaming job 1694990020000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:33:50 ERROR JobScheduler: Error running job streaming job 1694990030000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:34:00 ERROR JobScheduler: Error running job streaming job 1694990040000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:34:10 ERROR JobScheduler: Error running job streaming job 1694990050000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:34:12 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:34:12 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:34:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:34:12 WARN BlockManager: Block input-0-1694990052000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:34:20 ERROR JobScheduler: Error running job streaming job 1694990060000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:34:30 ERROR JobScheduler: Error running job streaming job 1694990070000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:34:40 ERROR JobScheduler: Error running job streaming job 1694990080000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:34:44 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:34:44 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:34:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:34:44 WARN BlockManager: Block input-0-1694990084200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:34:50 ERROR JobScheduler: Error running job streaming job 1694990090000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:35:00 ERROR JobScheduler: Error running job streaming job 1694990100000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:35:10 ERROR JobScheduler: Error running job streaming job 1694990110000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:35:16 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:35:16 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:35:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:35:16 WARN BlockManager: Block input-0-1694990116200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:35:20 ERROR JobScheduler: Error running job streaming job 1694990120000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:35:30 ERROR JobScheduler: Error running job streaming job 1694990130000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:35:40 ERROR JobScheduler: Error running job streaming job 1694990140000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:35:48 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:35:48 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:35:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:35:48 WARN BlockManager: Block input-0-1694990148200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:35:50 ERROR JobScheduler: Error running job streaming job 1694990150000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:00 ERROR JobScheduler: Error running job streaming job 1694990160000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:10 ERROR JobScheduler: Error running job streaming job 1694990170000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:20 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:36:20 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:36:20 ERROR JobScheduler: Error running job streaming job 1694990180000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:36:20 WARN BlockManager: Block input-0-1694990180200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:36:30 ERROR JobScheduler: Error running job streaming job 1694990190000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:40 ERROR JobScheduler: Error running job streaming job 1694990200000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:50 ERROR JobScheduler: Error running job streaming job 1694990210000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:36:52 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:36:52 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:36:52 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:36:52 WARN BlockManager: Block input-0-1694990212200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:37:00 ERROR JobScheduler: Error running job streaming job 1694990220000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:37:10 ERROR JobScheduler: Error running job streaming job 1694990230000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:37:20 ERROR JobScheduler: Error running job streaming job 1694990240000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:37:24 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:37:24 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:37:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:37:24 WARN BlockManager: Block input-0-1694990244200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:37:30 ERROR JobScheduler: Error running job streaming job 1694990250000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:37:40 ERROR JobScheduler: Error running job streaming job 1694990260000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:37:50 ERROR JobScheduler: Error running job streaming job 1694990270000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:37:56 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:37:56 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:37:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:37:56 WARN BlockManager: Block input-0-1694990276200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:38:00 ERROR JobScheduler: Error running job streaming job 1694990280000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:38:10 ERROR JobScheduler: Error running job streaming job 1694990290000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:38:20 ERROR JobScheduler: Error running job streaming job 1694990300000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:38:28 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:38:28 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:38:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:38:28 WARN BlockManager: Block input-0-1694990308400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:38:30 ERROR JobScheduler: Error running job streaming job 1694990310000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:38:40 ERROR JobScheduler: Error running job streaming job 1694990320000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:38:50 ERROR JobScheduler: Error running job streaming job 1694990330000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:39:00 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:39:00 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:39:00 ERROR JobScheduler: Error running job streaming job 1694990340000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:39:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:39:00 WARN BlockManager: Block input-0-1694990340400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:39:10 ERROR JobScheduler: Error running job streaming job 1694990350000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:39:20 ERROR JobScheduler: Error running job streaming job 1694990360000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:39:30 ERROR JobScheduler: Error running job streaming job 1694990370000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:39:32 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:39:32 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:39:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:39:32 WARN BlockManager: Block input-0-1694990372400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:39:40 ERROR JobScheduler: Error running job streaming job 1694990380000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:39:50 ERROR JobScheduler: Error running job streaming job 1694990390000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:40:00 ERROR JobScheduler: Error running job streaming job 1694990400000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:40:04 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:40:04 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:40:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:40:04 WARN BlockManager: Block input-0-1694990404400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:40:10 ERROR JobScheduler: Error running job streaming job 1694990410000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:40:20 ERROR JobScheduler: Error running job streaming job 1694990420000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:40:30 ERROR JobScheduler: Error running job streaming job 1694990430000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:40:36 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:40:36 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:40:36 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:40:36 WARN BlockManager: Block input-0-1694990436400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:40:40 ERROR JobScheduler: Error running job streaming job 1694990440000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:40:50 ERROR JobScheduler: Error running job streaming job 1694990450000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:41:00 ERROR JobScheduler: Error running job streaming job 1694990460000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:41:08 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:41:08 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:41:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:41:08 WARN BlockManager: Block input-0-1694990468400 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:41:10 ERROR JobScheduler: Error running job streaming job 1694990470000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:41:20 ERROR JobScheduler: Error running job streaming job 1694990480000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:41:30 ERROR JobScheduler: Error running job streaming job 1694990490000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:41:40 ERROR JobScheduler: Error running job streaming job 1694990500000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:41:40 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:41:40 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:41:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:41:40 WARN BlockManager: Block input-0-1694990500600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:41:50 ERROR JobScheduler: Error running job streaming job 1694990510000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:42:00 ERROR JobScheduler: Error running job streaming job 1694990520000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:42:10 ERROR JobScheduler: Error running job streaming job 1694990530000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:42:12 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:42:12 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:42:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:42:12 WARN BlockManager: Block input-0-1694990532600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:42:20 ERROR JobScheduler: Error running job streaming job 1694990540000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:42:30 ERROR JobScheduler: Error running job streaming job 1694990550000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:42:40 ERROR JobScheduler: Error running job streaming job 1694990560000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:42:44 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:42:44 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:42:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:42:44 WARN BlockManager: Block input-0-1694990564600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:42:50 ERROR JobScheduler: Error running job streaming job 1694990570000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:43:00 ERROR JobScheduler: Error running job streaming job 1694990580000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:43:10 ERROR JobScheduler: Error running job streaming job 1694990590000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:43:16 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:43:16 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:43:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:43:16 WARN BlockManager: Block input-0-1694990596600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:43:20 ERROR JobScheduler: Error running job streaming job 1694990600000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:43:30 ERROR JobScheduler: Error running job streaming job 1694990610000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:43:40 ERROR JobScheduler: Error running job streaming job 1694990620000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:43:48 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:43:48 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:43:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:43:48 WARN BlockManager: Block input-0-1694990628600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:43:50 ERROR JobScheduler: Error running job streaming job 1694990630000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:00 ERROR JobScheduler: Error running job streaming job 1694990640000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:10 ERROR JobScheduler: Error running job streaming job 1694990650000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:20 ERROR JobScheduler: Error running job streaming job 1694990660000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:20 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:44:20 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:44:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:44:20 WARN BlockManager: Block input-0-1694990660600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:44:30 ERROR JobScheduler: Error running job streaming job 1694990670000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:40 ERROR JobScheduler: Error running job streaming job 1694990680000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:50 ERROR JobScheduler: Error running job streaming job 1694990690000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:44:52 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:44:52 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:44:52 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:44:52 WARN BlockManager: Block input-0-1694990692600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:45:00 ERROR JobScheduler: Error running job streaming job 1694990700000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:45:10 ERROR JobScheduler: Error running job streaming job 1694990710000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:45:20 ERROR JobScheduler: Error running job streaming job 1694990720000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:45:24 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:45:24 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:45:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:45:24 WARN BlockManager: Block input-0-1694990724600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:45:30 ERROR JobScheduler: Error running job streaming job 1694990730000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:45:40 ERROR JobScheduler: Error running job streaming job 1694990740000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:45:50 ERROR JobScheduler: Error running job streaming job 1694990750000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:45:56 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:45:56 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:45:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:45:57 WARN BlockManager: Block input-0-1694990756800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:46:00 ERROR JobScheduler: Error running job streaming job 1694990760000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:46:10 ERROR JobScheduler: Error running job streaming job 1694990770000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:46:20 ERROR JobScheduler: Error running job streaming job 1694990780000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:46:28 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:46:28 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:46:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:46:29 WARN BlockManager: Block input-0-1694990788800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:46:30 ERROR JobScheduler: Error running job streaming job 1694990790000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:46:40 ERROR JobScheduler: Error running job streaming job 1694990800000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:46:50 ERROR JobScheduler: Error running job streaming job 1694990810000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:47:00 ERROR JobScheduler: Error running job streaming job 1694990820000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:47:00 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:47:00 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:47:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:47:01 WARN BlockManager: Block input-0-1694990820800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:47:10 ERROR JobScheduler: Error running job streaming job 1694990830000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:47:20 ERROR JobScheduler: Error running job streaming job 1694990840000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:47:30 ERROR JobScheduler: Error running job streaming job 1694990850000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:47:32 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:47:32 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:47:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:47:33 WARN BlockManager: Block input-0-1694990852800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:47:40 ERROR JobScheduler: Error running job streaming job 1694990860000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:47:50 ERROR JobScheduler: Error running job streaming job 1694990870000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:48:00 ERROR JobScheduler: Error running job streaming job 1694990880000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:48:04 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:48:04 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:48:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:48:05 WARN BlockManager: Block input-0-1694990884800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:48:10 ERROR JobScheduler: Error running job streaming job 1694990890000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:48:20 ERROR JobScheduler: Error running job streaming job 1694990900000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:48:30 ERROR JobScheduler: Error running job streaming job 1694990910000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:48:36 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:48:36 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:48:37 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:48:37 WARN BlockManager: Block input-0-1694990916800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:48:40 ERROR JobScheduler: Error running job streaming job 1694990920000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:48:50 ERROR JobScheduler: Error running job streaming job 1694990930000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:49:00 ERROR JobScheduler: Error running job streaming job 1694990940000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:49:08 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:49:08 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:49:09 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:49:09 WARN BlockManager: Block input-0-1694990948800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:49:10 ERROR JobScheduler: Error running job streaming job 1694990950000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:49:20 ERROR JobScheduler: Error running job streaming job 1694990960000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:49:30 ERROR JobScheduler: Error running job streaming job 1694990970000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:49:40 ERROR JobScheduler: Error running job streaming job 1694990980000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/09/17 17:49:40 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/09/17 17:49:40 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/09/17 17:49:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/09/17 17:49:41 WARN BlockManager: Block input-0-1694990980800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/09/17 17:49:50 ERROR JobScheduler: Error running job streaming job 1694990990000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_49918/2231400638.py\", line 6, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count'))\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/juan/2023/Spark-Tutorials/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    time.sleep(3)\n",
    "    top_10_tweets = sqlContext.sql('Select tag, count from tweets')\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    sns.plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x='count', y='tag', data=top_10_df)\n",
    "    sns.plt.show()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
